#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
å•†ä¸šçº§Live2Då£å‹åŒæ­¥å¼•æ“ V2.0
å®Œæ•´å®ç°ï¼šKalmanæ»¤æ³¢ + éŸ³ç´ è¯†åˆ« + æƒ…æ„Ÿè”åŠ¨ + 60FPSä¼˜åŒ–
"""

import numpy as np
import logging
import time
import math
from typing import Dict, Tuple, Optional, List, Any
from dataclasses import dataclass
from enum import Enum

# å°è¯•å¯¼å…¥scipyï¼Œå¦‚æœå¤±è´¥åˆ™æä¾›é™çº§æ–¹æ¡ˆ
try:
    from scipy import signal
    from scipy.fft import fft, fftfreq
    SCIPY_AVAILABLE = True
    logger = logging.getLogger(__name__)
    logger.info("scipyå¯ç”¨ï¼Œå¯ç”¨é«˜çº§å£å‹åŒæ­¥åŠŸèƒ½ï¼ˆFFT/LPC/å…±æŒ¯å³°æ£€æµ‹ï¼‰")
except ImportError:
    SCIPY_AVAILABLE = False
    logger = logging.getLogger(__name__)
    logger.warning("scipyä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€åŒ–çš„å£å‹åŒæ­¥ï¼ˆä»…åŸºäºéŸ³é‡ï¼‰")
    # ä½¿ç”¨numpyçš„FFTä½œä¸ºé™çº§æ–¹æ¡ˆ
    from numpy.fft import fft, fftfreq
    signal = None  # æ ‡è®°ä¸ºä¸å¯ç”¨


class EmotionType(Enum):
    """æƒ…æ„Ÿç±»å‹æšä¸¾"""
    NEUTRAL = "neutral"
    HAPPY = "happy"
    SAD = "sad"
    ANGRY = "angry"
    SURPRISED = "surprised"
    QUESTIONING = "questioning"


@dataclass
class LipSyncState:
    """å£å‹åŒæ­¥çŠ¶æ€"""
    mouth_open: float = 0.0
    mouth_form: float = 0.0
    mouth_smile: float = 0.0
    eye_brow_up: float = 0.0
    eye_wide: float = 0.0
    current_viseme: str = 'silence'
    current_emotion: EmotionType = EmotionType.NEUTRAL
    timestamp: float = 0.0


# æ³¨é‡Šï¼šKalmanFilter å’Œ SpringDamperSystem ç±»å·²è¢«ç§»é™¤
# åŸå› ï¼šè¿™äº›ç±»åœ¨å®é™…ä½¿ç”¨ä¸­è¢«æ›¿æ¢ä¸ºç®€å•çš„æŒ‡æ•°å¹³æ»‘ç®—æ³•
# SpringDamperSystem ä¼šå¯¼è‡´æ•°å€¼çˆ†ç‚¸é—®é¢˜
# ç°åœ¨ä½¿ç”¨ exponential smoothing (alpha blending) æ›¿ä»£ï¼Œè§ process_audio_chunk æ–¹æ³•


class AdvancedLipSyncEngineV2:
    """
    å•†ä¸šçº§Live2Då£å‹åŒæ­¥å¼•æ“ V2.0
    
    æ ¸å¿ƒç‰¹æ€§ï¼š
    1. å¡å°”æ›¼æ»¤æ³¢é¢„æµ‹æ€§å¹³æ»‘
    2. å¼¹ç°§-é˜»å°¼ç‰©ç†æ¨¡æ‹Ÿ
    3. é«˜çº§éŸ³ç´ è¯†åˆ«ï¼ˆMELé¢‘è°± + å…±æŒ¯å³°ï¼‰
    4. æƒ…æ„Ÿè¡¨æƒ…è”åŠ¨
    5. 60FPSä¼˜åŒ–æ›´æ–°
    6. è‡ªé€‚åº”å‚æ•°è°ƒæ•´
    """
    
    # éŸ³ç´ å‚æ•°é¢„è®¾ï¼ˆå®æ—¶è¯­éŸ³å’ŒEdgeTTSç»Ÿä¸€å‚æ•°ï¼‰
    VISEME_PARAMS = {
        # å…ƒéŸ³ - é€‚ä¸­å¹…åº¦
        'a': {'mouth_open': 0.70, 'mouth_form': 0.0, 'mouth_smile': 0.05, 'name': 'å•Š[a]'},
        'i': {'mouth_open': 0.25, 'mouth_form': -0.68, 'mouth_smile': 0.38, 'name': 'è¡£[i]'},
        'u': {'mouth_open': 0.32, 'mouth_form': 0.72, 'mouth_smile': -0.18, 'name': 'ä¹Œ[u]'},
        'e': {'mouth_open': 0.45, 'mouth_form': -0.25, 'mouth_smile': 0.18, 'name': 'è¯¶[e]'},
        'o': {'mouth_open': 0.58, 'mouth_form': 0.58, 'mouth_smile': 0.0, 'name': 'å“¦[o]'},

        # è¾…éŸ³
        'consonant': {'mouth_open': 0.28, 'mouth_form': 0.0, 'mouth_smile': 0.0, 'name': 'è¾…éŸ³'},
        'sibilant': {'mouth_open': 0.18, 'mouth_form': -0.28, 'mouth_smile': 0.22, 'name': 'é½¿éŸ³[s/sh/z]'},
        'm_n': {'mouth_open': 0.06, 'mouth_form': 0.0, 'mouth_smile': 0.0, 'name': 'é—­éŸ³[m/n]'},
        'plosive': {'mouth_open': 0.22, 'mouth_form': 0.0, 'mouth_smile': 0.0, 'name': 'çˆ†ç ´éŸ³[p/b/t/d/k/g]'},

        # ç‰¹æ®Š
        'silence': {'mouth_open': 0.0, 'mouth_form': 0.0, 'mouth_smile': 0.0, 'name': 'é™éŸ³'},
    }
    
    # æƒ…æ„Ÿå‚æ•°é¢„è®¾
    EMOTION_PARAMS = {
        EmotionType.NEUTRAL: {
            'mouth_smile': 0.0,
            'eye_brow_up': 0.0,
            'eye_wide': 0.0,
        },
        EmotionType.HAPPY: {
            'mouth_smile': 0.6,
            'eye_brow_up': 0.1,
            'eye_wide': 0.0,
        },
        EmotionType.SAD: {
            'mouth_smile': -0.4,
            'eye_brow_up': -0.3,
            'eye_wide': 0.0,
        },
        EmotionType.ANGRY: {
            'mouth_smile': -0.3,
            'eye_brow_up': -0.5,
            'eye_wide': 0.2,
        },
        EmotionType.SURPRISED: {
            'mouth_smile': 0.0,
            'eye_brow_up': 0.7,
            'eye_wide': 0.8,
        },
        EmotionType.QUESTIONING: {
            'mouth_smile': 0.2,
            'eye_brow_up': 0.4,
            'eye_wide': 0.1,
        },
    }
    
    def __init__(self, sample_rate: int = 24000, target_fps: int = 60):
        """
        åˆå§‹åŒ–å¼•æ“

        Args:
            sample_rate: éŸ³é¢‘é‡‡æ ·ç‡
            target_fps: ç›®æ ‡æ›´æ–°å¸§ç‡
        """
        self.sample_rate = sample_rate
        self.target_fps = target_fps
        self.frame_time = 1.0 / target_fps

        # æ³¨é‡Šï¼šKalmanæ»¤æ³¢å™¨å’Œå¼¹ç°§-é˜»å°¼ç³»ç»Ÿå·²è¢«ç§»é™¤
        # åŸå› ï¼šSpringDamperSystemä¼šå¯¼è‡´æ•°å€¼çˆ†ç‚¸ï¼ŒKalmanè¿‡äºå¤æ‚
        # ç°ä½¿ç”¨ç®€å•çš„æŒ‡æ•°å¹³æ»‘ç®—æ³•ï¼ˆalpha blendingï¼‰ï¼Œåœ¨process_audio_chunkä¸­å®ç°

        # å½“å‰çŠ¶æ€
        self.state = LipSyncState()

        # ç›®æ ‡å€¼
        self.target_mouth_open = 0.0
        self.target_mouth_form = 0.0
        self.target_mouth_smile = 0.0

        # æƒ…æ„ŸçŠ¶æ€
        self.current_emotion = EmotionType.NEUTRAL
        self.emotion_intensity = 1.0

        # å†å²è®°å½•ï¼ˆç”¨äºè‡ªé€‚åº”ï¼‰
        self.volume_history: List[float] = []
        self.max_history_size = 100

        # åŠ¨æ€é˜ˆå€¼ï¼ˆEdgeTTSä¼˜åŒ–ï¼‰
        self.silence_threshold = 50  # é™éŸ³é˜ˆå€¼ï¼Œè¿‡æ»¤èƒŒæ™¯å™ªéŸ³
        self.adaptive_volume_scale = 2000.0  # éŸ³é‡ç¼©æ”¾ç³»æ•°ï¼ŒEdgeTTSä½¿ç”¨

        # æŒç»­æ—¶é—´æ§åˆ¶ï¼ˆé˜²æ­¢è¿‡å¿«é—­å˜´ï¼‰
        self.last_sound_time = 0.0
        self.sound_persistence = 0.15  # å£°éŸ³æŒç»­150msï¼Œé˜²æ­¢è¿‡å¿«é—­å˜´

        # æ³¨é‡Šï¼šéŸ³ç´ ç¨³å®šæ€§æ§åˆ¶å’Œå†å²è¿½è¸ªå·²è¢«ç§»é™¤
        # åŸå› ï¼š_stabilize_viseme æ–¹æ³•ä¼šå¯¼è‡´éŸ³ç´ åˆ‡æ¢å»¶è¿Ÿï¼ˆéœ€è¦2å¸§ç¡®è®¤ï¼‰
        # å®æ—¶è¯­éŸ³ä¸­éŸ³ç´ å˜åŒ–å¿«ï¼Œç¨³å®šæ€§æ£€æŸ¥åè€Œé™ä½å“åº”é€Ÿåº¦
        # ç°åœ¨ç›´æ¥ä½¿ç”¨åŸå§‹è¯†åˆ«ç»“æœï¼Œé€šè¿‡æŒ‡æ•°å¹³æ»‘ç®—æ³•å¤„ç†æŠ–åŠ¨

        # æ€§èƒ½ç»Ÿè®¡
        self.last_update_time = time.time()
        self.frame_count = 0
        self.avg_fps = 0.0

        logger.info(f"AdvancedLipSyncEngineV2 åˆå§‹åŒ–å®Œæˆ (ç›®æ ‡{target_fps}FPS)")
    
    def process_audio_chunk(self, audio_chunk: bytes) -> Dict[str, float]:
        """
        å¤„ç†éŸ³é¢‘å—ï¼Œè¿”å›Live2Då‚æ•°ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            audio_chunk: PCM 16bit éŸ³é¢‘æ•°æ®
            
        Returns:
            Live2Då‚æ•°å­—å…¸
        """
        try:
            current_time = time.time()
            dt = current_time - self.last_update_time
            self.last_update_time = current_time
            
            # è½¬æ¢éŸ³é¢‘æ•°æ®
            audio_array = np.frombuffer(audio_chunk, dtype=np.int16).astype(np.float32)

            # 1. è®¡ç®—RMSèƒ½é‡å’ŒZCR
            rms = self._calculate_rms(audio_array)
            zcr = self._calculate_zcr(audio_array)

            # 2. æ›´æ–°å†å²å¹¶è‡ªé€‚åº”
            self._update_volume_history(rms)

            if rms < self.silence_threshold:
                # é™éŸ³çŠ¶æ€
                self.target_mouth_open = 0.0
                self.target_mouth_form = 0.0
                self.state.current_viseme = 'silence'
                viseme = 'silence'
            else:
                # å®Œæ•´DSPå¤„ç†ï¼ˆæ¯å¸§éƒ½æ‰§è¡Œï¼Œç¡®ä¿åŒæ­¥ï¼‰
                # 3. é¢‘è°±åˆ†æ
                spectrum_features = self._analyze_spectrum_advanced(audio_array)

                # 4. å…±æŒ¯å³°æ£€æµ‹
                formants = self._detect_formants_lpc(audio_array)
                f1, f2 = formants

                # 5. åŸºé¢‘æ£€æµ‹
                f0 = self._detect_f0_autocorr(audio_array)

                # 6. éŸ³ç´ è¯†åˆ«ï¼ˆå¢å¼ºç‰ˆï¼Œä½¿ç”¨ZCRç‰¹å¾ï¼‰
                raw_viseme = self._identify_viseme_advanced(spectrum_features, formants, f0, rms, zcr)

                # ç›´æ¥ä½¿ç”¨åŸå§‹éŸ³ç´ 
                viseme = raw_viseme
                self.state.current_viseme = viseme

                # 7. è·å–ç›®æ ‡å‚æ•°
                params = self.VISEME_PARAMS.get(viseme, self.VISEME_PARAMS['silence'])

                # 8. èƒ½é‡è°ƒåˆ¶
                energy_factor = np.clip(rms / self.adaptive_volume_scale, 0.3, 1.0)

                self.target_mouth_open = params['mouth_open'] * energy_factor
                self.target_mouth_form = params['mouth_form']
                self.target_mouth_smile = params.get('mouth_smile', 0.0)

            # 9. ç®€åŒ–å¹³æ»‘å¤„ç†ï¼ˆå¿«é€Ÿå“åº”ï¼Œå‡å°‘å»¶è¿Ÿï¼‰
            # ä½¿ç”¨æ›´å¤§çš„alphaå€¼ï¼ˆ0.5-0.6ï¼‰ï¼Œå¿«é€Ÿè·Ÿè¸ªç›®æ ‡å€¼
            # å¹³è¡¡å¹³æ»‘åº¦å’Œå“åº”é€Ÿåº¦
            alpha_open = 0.6  # æé«˜åˆ°0.6ï¼Œæ›´å¿«å“åº”ï¼ˆåŸ0.3ï¼‰
            alpha_form = 0.5  # æé«˜åˆ°0.5ï¼Œæ›´å¿«å“åº”ï¼ˆåŸ0.25ï¼‰

            if not hasattr(self, '_smooth_mouth_open'):
                self._smooth_mouth_open = self.target_mouth_open
                self._smooth_mouth_form = self.target_mouth_form

            self._smooth_mouth_open += (self.target_mouth_open - self._smooth_mouth_open) * alpha_open
            self._smooth_mouth_form += (self.target_mouth_form - self._smooth_mouth_form) * alpha_form

            final_mouth_open = self._smooth_mouth_open
            final_mouth_form = self._smooth_mouth_form
            final_mouth_smile = self.target_mouth_smile
            
            # 11. åº”ç”¨æƒ…æ„Ÿè°ƒåˆ¶
            emotion_modulation = self._get_emotion_modulation()
            final_mouth_smile += emotion_modulation['mouth_smile']

            # æœ€ç»ˆå‚æ•°è¾“å‡ºæ—¥å¿—ï¼ˆDEBUGçº§åˆ«ï¼‰
            if self.frame_count % 10 == 0 and logger.isEnabledFor(logging.DEBUG):
                logger.debug(
                    f"ğŸ“Š [æœ€ç»ˆè¾“å‡º] å¼ å¼€={final_mouth_open:.3f} å˜´å½¢={final_mouth_form:.3f} å¾®ç¬‘={final_mouth_smile:.3f}"
                )
            
            # 10. ç¦ç”¨è‡ªç„¶æŠ–åŠ¨ï¼ˆå®æ—¶è¯­éŸ³ä¸­æŠ–åŠ¨ä¼šå¢åŠ è§†è§‰å»¶è¿Ÿæ„Ÿï¼‰
            # ç›´æ¥ä½¿ç”¨å¹³æ»‘åçš„å€¼ï¼Œä¸æ·»åŠ éšæœºæŠ–åŠ¨
            final_params = {
                'mouth_open': np.clip(final_mouth_open, 0.0, 1.0),
                'mouth_form': np.clip(final_mouth_form, -1.0, 1.0),
                'mouth_smile': np.clip(final_mouth_smile, -1.0, 1.0),
            }

            # 13. æ·»åŠ æƒ…æ„Ÿå‚æ•°
            final_params.update({
                'eye_brow_up': emotion_modulation['eye_brow_up'],
                'eye_wide': emotion_modulation['eye_wide'],
            })
            
            # æ›´æ–°çŠ¶æ€
            self.state.mouth_open = final_params['mouth_open']
            self.state.mouth_form = final_params['mouth_form']
            self.state.mouth_smile = final_params['mouth_smile']
            self.state.timestamp = current_time
            
            # æ›´æ–°FPSç»Ÿè®¡
            self._update_fps_stats()
            
            return final_params
            
        except Exception as e:
            logger.debug(f"éŸ³é¢‘å¤„ç†é”™è¯¯: {e}")
            return {'mouth_open': 0.0, 'mouth_form': 0.0, 'mouth_smile': 0.0}
    
    def _calculate_rms(self, audio: np.ndarray) -> float:
        """è®¡ç®—RMSèƒ½é‡"""
        return float(np.sqrt(np.mean(audio ** 2)))

    def _calculate_zcr(self, audio: np.ndarray) -> float:
        """
        è®¡ç®—è¿‡é›¶ç‡(Zero Crossing Rate)

        è¿‡é›¶ç‡æ˜¯éŸ³é¢‘ä¿¡å·åœ¨å•ä½æ—¶é—´å†…ç©¿è¶Šé›¶ç‚¹çš„æ¬¡æ•°ï¼Œæ˜¯åŒºåˆ†æ¸…éŸ³å’ŒæµŠéŸ³çš„é‡è¦ç‰¹å¾ã€‚

        ç‰¹å¾è§£é‡Šï¼š
        - é«˜ZCR (>0.3): æ¸…éŸ³/é½¿éŸ³ï¼ˆs, sh, f, thç­‰ï¼‰
        - ä½ZCR (<0.15): æµŠéŸ³/é¼»éŸ³ï¼ˆå…ƒéŸ³, m, n, lç­‰ï¼‰
        - ä¸­ç­‰ZCR (0.15-0.3): è¿‡æ¸¡éŸ³æˆ–æ··åˆéŸ³

        Returns:
            ZCRå€¼ (0.0-1.0èŒƒå›´)
        """
        if len(audio) < 2:
            return 0.0

        # è®¡ç®—ç¬¦å·å˜åŒ–æ¬¡æ•°
        signs = np.sign(audio)
        sign_changes = np.diff(signs)

        # è¿‡é›¶ç‡ = ç¬¦å·å˜åŒ–æ¬¡æ•° / (2 * æ ·æœ¬æ•°)
        zcr = np.sum(np.abs(sign_changes)) / (2.0 * len(audio))

        return float(zcr)
    
    def _update_volume_history(self, rms: float):
        """æ›´æ–°éŸ³é‡å†å²å¹¶è‡ªé€‚åº”è°ƒæ•´"""
        self.volume_history.append(rms)
        if len(self.volume_history) > self.max_history_size:
            self.volume_history.pop(0)

        # è‡ªé€‚åº”è°ƒæ•´é˜ˆå€¼ï¼ˆåŸºäºæœ€è¿‘çš„éŸ³é‡åˆ†å¸ƒï¼‰
        if len(self.volume_history) >= 20:
            percentile_95 = np.percentile(self.volume_history, 95)
            self.adaptive_volume_scale = max(1000.0, percentile_95 * 1.2)

    # æ³¨é‡Šï¼šå·²ç§»é™¤ä»¥ä¸‹æœªä½¿ç”¨çš„æ–¹æ³•ï¼ˆçº¦170è¡Œä»£ç ï¼‰ï¼š
    # - _stabilize_viseme(): éŸ³ç´ ç¨³å®šæ€§æ£€æŸ¥ï¼Œå¯¼è‡´å»¶è¿Ÿï¼Œå·²åœ¨process_audio_chunkä¸­ç›´æ¥ä½¿ç”¨åŸå§‹éŸ³ç´ 
    # - _track_viseme_changes(): éŸ³ç´ å†å²è¿½è¸ªï¼Œç”¨äºè¯Šæ–­ï¼Œä½†ä»æœªè¢«è°ƒç”¨
    # - _add_natural_variation(): æ·»åŠ è‡ªç„¶æŠ–åŠ¨ï¼Œå·²ç¦ç”¨ï¼Œç›´æ¥ä½¿ç”¨np.clipæ›¿ä»£

    def _analyze_spectrum_advanced(self, audio: np.ndarray) -> Dict[str, float]:
        """
        é«˜çº§é¢‘è°±åˆ†æï¼ˆMELé¢‘è°±ï¼‰

        æ€§èƒ½è¯´æ˜ï¼šæ­¤æ–¹æ³•æ‰§è¡Œæ ‡å‡†FFTç”¨äºé¢‘è°±åˆ†æã€‚
        æ³¨æ„ï¼šå…±æŒ¯å³°æ£€æµ‹éœ€è¦å•ç‹¬çš„é¢„åŠ é‡FFTï¼Œç›®å‰æ— æ³•åˆå¹¶ã€‚
        æœªæ¥ä¼˜åŒ–æ–¹å‘ï¼šå¯è€ƒè™‘ç¼“å­˜FFTç»“æœæˆ–ä½¿ç”¨æ›´å¿«çš„FFTå®ç°ã€‚

        Returns:
            é¢‘è°±ç‰¹å¾å­—å…¸
        """
        try:
            n = len(audio)
            if n < 512:
                audio = np.pad(audio, (0, 512 - n), 'constant')
                n = 512
            
            # åº”ç”¨æ±‰æ˜çª—
            windowed = audio * np.hamming(len(audio))
            
            # FFTå˜æ¢
            spectrum = fft(windowed)
            freqs = fftfreq(len(spectrum), 1/self.sample_rate)
            magnitude = np.abs(spectrum[:n//2])
            freqs = freqs[:n//2]
            
            # MELé¢‘ç‡è½¬æ¢
            mel_bands = self._convert_to_mel_scale(freqs, magnitude)
            
            # åˆ†é¢‘æ®µèƒ½é‡
            features = {
                'low_energy': np.sum(mel_bands[:20]) / np.sum(mel_bands),  # ä½é¢‘
                'mid_energy': np.sum(mel_bands[20:50]) / np.sum(mel_bands),  # ä¸­é¢‘
                'high_energy': np.sum(mel_bands[50:]) / np.sum(mel_bands),  # é«˜é¢‘
            }
            
            # é¢‘è°±è´¨å¿ƒ
            if np.sum(magnitude) > 0:
                features['spectral_centroid'] = np.sum(freqs * magnitude) / np.sum(magnitude)
            else:
                features['spectral_centroid'] = 0
            
            # é¢‘è°±å¹³å¦åº¦ï¼ˆè¯†åˆ«æ¸…éŸ³ï¼‰
            geometric_mean = np.exp(np.mean(np.log(magnitude + 1e-10)))
            arithmetic_mean = np.mean(magnitude)
            features['spectral_flatness'] = geometric_mean / (arithmetic_mean + 1e-10)
            
            return features
            
        except Exception as e:
            logger.debug(f"é¢‘è°±åˆ†æé”™è¯¯: {e}")
            return {'low_energy': 0, 'mid_energy': 0, 'high_energy': 0, 'spectral_centroid': 0, 'spectral_flatness': 0}
    
    def _convert_to_mel_scale(self, freqs: np.ndarray, magnitude: np.ndarray, n_mels: int = 80) -> np.ndarray:
        """è½¬æ¢åˆ°MELé¢‘ç‡å°ºåº¦"""
        def hz_to_mel(hz):
            return 2595 * np.log10(1 + hz / 700)
        
        def mel_to_hz(mel):
            return 700 * (10 ** (mel / 2595) - 1)
        
        # åˆ›å»ºMELæ»¤æ³¢å™¨ç»„
        mel_min = hz_to_mel(80)
        mel_max = hz_to_mel(8000)
        mel_points = np.linspace(mel_min, mel_max, n_mels + 2)
        hz_points = mel_to_hz(mel_points)
        
        # è®¡ç®—æ¯ä¸ªMELé¢‘æ®µçš„èƒ½é‡
        mel_bands = np.zeros(n_mels)
        for i in range(n_mels):
            # æ‰¾åˆ°å¯¹åº”çš„é¢‘ç‡èŒƒå›´
            mask = (freqs >= hz_points[i]) & (freqs <= hz_points[i + 2])
            if np.any(mask):
                mel_bands[i] = np.sum(magnitude[mask])
        
        return mel_bands
    
    def _detect_formants_lpc(self, audio: np.ndarray) -> Tuple[float, float]:
        """
        LPCçº¿æ€§é¢„æµ‹ç¼–ç å…±æŒ¯å³°æ£€æµ‹ï¼ˆå¢å¼ºç‰ˆï¼‰

        Returns:
            (F1, F2) å…±æŒ¯å³°é¢‘ç‡
        """
        try:
            # å¦‚æœscipyä¸å¯ç”¨ï¼Œè¿”å›é»˜è®¤å€¼
            if not SCIPY_AVAILABLE or signal is None:
                return 0.0, 0.0

            if len(audio) < 256:
                return 0.0, 0.0

            # é¢„åŠ é‡
            pre_emphasis = 0.97
            emphasized = np.append(audio[0], audio[1:] - pre_emphasis * audio[:-1])

            # FFTå³°å€¼æ£€æµ‹æ³•ï¼ˆç®€åŒ–ä½†æœ‰æ•ˆï¼‰
            spectrum = np.abs(fft(emphasized * np.hamming(len(emphasized))))
            freqs = fftfreq(len(spectrum), 1/self.sample_rate)

            # åªçœ‹æ­£é¢‘ç‡
            pos_freqs = freqs[:len(freqs)//2]
            pos_spectrum = spectrum[:len(spectrum)//2]

            # å¹³æ»‘é¢‘è°±
            smoothed = signal.savgol_filter(pos_spectrum, 11, 3)

            # æ‰¾å³°å€¼
            peaks, properties = signal.find_peaks(smoothed, height=np.max(smoothed)*0.15, distance=10)

            if len(peaks) >= 2:
                # æŒ‰èƒ½é‡æ’åº
                peak_heights = properties['peak_heights']
                sorted_indices = np.argsort(peak_heights)[::-1]
                top_peaks = peaks[sorted_indices[:2]]

                # æŒ‰é¢‘ç‡æ’åºå¾—åˆ°F1, F2
                top_peaks_sorted = np.sort(top_peaks)
                f1 = float(pos_freqs[top_peaks_sorted[0]])
                f2 = float(pos_freqs[top_peaks_sorted[1]])

                # é™åˆ¶åœ¨åˆç†èŒƒå›´
                f1 = np.clip(f1, 200, 1000)
                f2 = np.clip(f2, 800, 3000)

                return f1, f2

            return 0.0, 0.0
            
        except Exception as e:
            logger.debug(f"å…±æŒ¯å³°æ£€æµ‹é”™è¯¯: {e}")
            return 0.0, 0.0
    
    def _detect_f0_autocorr(self, audio: np.ndarray) -> float:
        """
        è‡ªç›¸å…³æ³•åŸºé¢‘æ£€æµ‹ï¼ˆå¢å¼ºç‰ˆï¼‰

        Returns:
            åŸºé¢‘F0
        """
        try:
            # è‡ªç›¸å…³
            correlation = np.correlate(audio, audio, mode='full')
            correlation = correlation[len(correlation)//2:]

            # å½’ä¸€åŒ–
            correlation = correlation / correlation[0] if correlation[0] > 0 else correlation

            # é™åˆ¶æœç´¢èŒƒå›´ (80-400Hz)
            min_period = int(self.sample_rate / 400)
            max_period = int(self.sample_rate / 80)

            search_range = correlation[min_period:max_period]

            if len(search_range) > 0:
                # å¦‚æœscipyå¯ç”¨ï¼Œä½¿ç”¨find_peaks
                if SCIPY_AVAILABLE and signal is not None:
                    peaks, _ = signal.find_peaks(search_range, height=0.3)

                    if len(peaks) > 0:
                        # é€‰æ‹©æœ€é«˜çš„å³°
                        best_peak = peaks[np.argmax(search_range[peaks])]
                        period = best_peak + min_period
                        f0 = self.sample_rate / period

                        # é™åˆ¶åœ¨åˆç†èŒƒå›´
                        f0 = np.clip(f0, 80, 400)
                        return float(f0)
                else:
                    # é™çº§æ–¹æ¡ˆï¼šç®€å•åœ°æ‰¾æœ€å¤§å€¼
                    best_peak = np.argmax(search_range)
                    period = best_peak + min_period
                    f0 = self.sample_rate / period

                    # é™åˆ¶åœ¨åˆç†èŒƒå›´
                    f0 = np.clip(f0, 80, 400)
                    return float(f0)

            return 0.0

        except Exception as e:
            logger.debug(f"åŸºé¢‘æ£€æµ‹é”™è¯¯: {e}")
            return 0.0
    
    def _identify_viseme_advanced(self, spectrum: Dict, formants: Tuple, f0: float, rms: float, zcr: float = 0.0) -> str:
        """
        å¢å¼ºç‰ˆéŸ³ç´ è¯†åˆ«ï¼ˆä½¿ç”¨ZCRç‰¹å¾ï¼‰

        åŸºäºå¤šç»´ç‰¹å¾ç»¼åˆåˆ¤æ–­

        Args:
            spectrum: é¢‘è°±ç‰¹å¾å­—å…¸
            formants: (F1, F2) å…±æŒ¯å³°é¢‘ç‡å…ƒç»„
            f0: åŸºé¢‘
            rms: éŸ³é¢‘èƒ½é‡
            zcr: è¿‡é›¶ç‡ï¼ˆæ–°å¢ï¼Œç”¨äºæ¸…éŸ³/æµŠéŸ³åˆ¤æ–­ï¼‰

        Returns:
            è¯†åˆ«çš„éŸ³ç´ /visemeå­—ç¬¦ä¸²
        """
        f1, f2 = formants

        # 1. é™éŸ³åˆ¤æ–­
        if rms < self.silence_threshold:
            return 'silence'

        # 2. åŸºäºZCRçš„æ¸…éŸ³åˆ¤æ–­ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        # é«˜ZCRé€šå¸¸è¡¨ç¤ºæ¸…éŸ³/é½¿éŸ³ï¼ˆs, sh, f, thç­‰ï¼‰
        if zcr > 0.3:
            return 'sibilant'

        # 3. åŸºäºé¢‘è°±å¹³å¦åº¦çš„æ¸…éŸ³åˆ¤æ–­ï¼ˆè¾…åŠ©ï¼‰
        if spectrum.get('spectral_flatness', 0) > 0.5:
            return 'sibilant'

        # 4. è¾…éŸ³åˆ¤æ–­
        high_energy = spectrum.get('high_energy', 0)
        mid_energy = spectrum.get('mid_energy', 0)

        if high_energy > 0.4:
            return 'sibilant'  # é½¿éŸ³ s/sh/z

        if mid_energy > 0.6:
            if rms < self.adaptive_volume_scale * 0.3:
                return 'm_n'  # é¼»éŸ³ m/n
            return 'plosive'  # çˆ†ç ´éŸ³

        # 5. å…ƒéŸ³åˆ¤æ–­ï¼ˆåŸºäºå…±æŒ¯å³°ï¼‰
        if f1 > 0 and f2 > 0:
            # F1/F2æ¯”å€¼åˆ†æ
            ratio = f1 / f2 if f2 > 0 else 0
            
            # å…ƒéŸ³åˆ†ç±»ï¼ˆåŸºäºå…±æŒ¯å³°ç©ºé—´ï¼‰
            if f1 > 700:  # ä½å…ƒéŸ³
                if f2 < 1400:
                    return 'o'  # åä½å…ƒéŸ³ [É”]
                elif f2 > 1400:
                    return 'a'  # å‰ä½å…ƒéŸ³ [a]
            elif f1 > 400:  # ä¸­å…ƒéŸ³
                if f2 > 2000:
                    return 'e'  # å‰ä¸­å…ƒéŸ³ [e]
                else:
                    return 'o'  # åä¸­å…ƒéŸ³ [o]
            else:  # é«˜å…ƒéŸ³ f1 < 400
                if f2 > 2200:
                    return 'i'  # å‰é«˜å…ƒéŸ³ [i]
                else:
                    return 'u'  # åé«˜å…ƒéŸ³ [u]
        
        # 5. åŸºäºé¢‘è°±è´¨å¿ƒçš„å¤‡ç”¨åˆ¤æ–­
        centroid = spectrum.get('spectral_centroid', 0)
        if centroid > 3000:
            return 'i'
        elif centroid > 1500:
            return 'e'
        elif centroid > 800:
            return 'a'
        else:
            return 'o'
    
    def _get_emotion_modulation(self) -> Dict[str, float]:
        """è·å–æƒ…æ„Ÿè°ƒåˆ¶å‚æ•°"""
        base_params = self.EMOTION_PARAMS.get(self.current_emotion, self.EMOTION_PARAMS[EmotionType.NEUTRAL])
        
        # åº”ç”¨æƒ…æ„Ÿå¼ºåº¦
        return {
            key: value * self.emotion_intensity
            for key, value in base_params.items()
        }

    def _update_fps_stats(self):
        """æ›´æ–°FPSç»Ÿè®¡"""
        self.frame_count += 1
        
        if self.frame_count % 60 == 0:
            current_time = time.time()
            elapsed = current_time - self.state.timestamp
            if elapsed > 0:
                self.avg_fps = 60 / elapsed
                if self.avg_fps < self.target_fps * 0.8:
                    logger.warning(f"FPSä½äºç›®æ ‡: {self.avg_fps:.1f}/{self.target_fps}")
    
    def set_emotion(self, emotion: EmotionType, intensity: float = 1.0):
        """
        è®¾ç½®æƒ…æ„ŸçŠ¶æ€
        
        Args:
            emotion: æƒ…æ„Ÿç±»å‹
            intensity: å¼ºåº¦ (0.0-1.0)
        """
        self.current_emotion = emotion
        self.emotion_intensity = np.clip(intensity, 0.0, 1.0)
        logger.debug(f"è®¾ç½®æƒ…æ„Ÿ: {emotion.value}, å¼ºåº¦: {intensity}")
    
    def detect_emotion_from_prosody(self, f0: float, rms: float, speech_rate: float = 1.0) -> EmotionType:
        """
        ä»éŸµå¾‹ç‰¹å¾æ£€æµ‹æƒ…æ„Ÿï¼ˆè‡ªåŠ¨æƒ…æ„Ÿè¯†åˆ«ï¼‰
        
        Args:
            f0: åŸºé¢‘
            rms: éŸ³é‡
            speech_rate: è¯­é€Ÿï¼ˆç›¸å¯¹å€¼ï¼‰
            
        Returns:
            æ£€æµ‹åˆ°çš„æƒ…æ„Ÿç±»å‹
        """
        # ç®€å•çš„è§„åˆ™åŸºæƒ…æ„Ÿè¯†åˆ«
        if f0 > 250 and rms > self.adaptive_volume_scale * 0.6:
            return EmotionType.SURPRISED  # é«˜éŸ³+å¤§å£° = æƒŠè®¶
        elif f0 > 200 and speech_rate > 1.2:
            return EmotionType.HAPPY  # é«˜éŸ³+å¿«é€Ÿ = å¼€å¿ƒ
        elif f0 < 150 and rms > self.adaptive_volume_scale * 0.5:
            return EmotionType.ANGRY  # ä½éŸ³+å¤§å£° = ç”Ÿæ°”
        elif f0 < 150 and rms < self.adaptive_volume_scale * 0.3:
            return EmotionType.SAD  # ä½éŸ³+å°å£° = æ‚²ä¼¤
        else:
            return EmotionType.NEUTRAL
    
    def reset(self):
        """é‡ç½®å¼•æ“çŠ¶æ€"""
        self.state = LipSyncState()

        # é‡ç½®å¹³æ»‘çŠ¶æ€
        if hasattr(self, '_smooth_mouth_open'):
            self._smooth_mouth_open = 0.0
        if hasattr(self, '_smooth_mouth_form'):
            self._smooth_mouth_form = 0.0

        # æ¸…ç©ºå†å²
        self.volume_history.clear()

        logger.info("[LipSync] å¼•æ“å·²é‡ç½®")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        return {
            'avg_fps': self.avg_fps,
            'target_fps': self.target_fps,
            'frame_count': self.frame_count,
            'current_viseme': self.state.current_viseme,
            'current_emotion': self.current_emotion.value,
            'adaptive_threshold': self.adaptive_volume_scale,
        }


# ä¾¿æ·å‡½æ•°
def create_advanced_lip_sync_engine(sample_rate: int = 24000, target_fps: int = 60) -> AdvancedLipSyncEngineV2:
    """åˆ›å»ºé«˜çº§å£å‹åŒæ­¥å¼•æ“å®ä¾‹"""
    return AdvancedLipSyncEngineV2(sample_rate=sample_rate, target_fps=target_fps)